\section{Related Work} \label{sec:rl}
%
%\paragraph{Data Storage.}
%For data storage we are mostly interested in compression tools with compression rates.
%There are many studies~\cite{alakuijala2015comparison,squash2016squash,mahoney2016large,lzturbo2016lzturbo,turbobench2016turbobench} evaluating general compression tools in regards to their compression rate, compression speed, and decompression speed.
%Classic encoders are zlib~\cite{deutsch1996zlib} (which utilizes LZ77~\cite{ziv1977universal}, LZ78~\cite{ziv1978compression}, LZF~\cite{lehmann2016liblzf}, LZO~\cite{oberhumer2005lzo}, LZW~\cite{welch1984technique}, and Huffmann codes~\cite{huffman1952method}), bzip2 (using run-length encoding, the Burrows-Wheeler transformation~\cite{burrows1994block}, move-to-front encoding, and delta encoding), and LZMA; LZMA is usually reported to provide best compression rates.
%
%\paragraph{Data Transmission.}
%
%Since major browsers natively support Zlib-based decompression, this method is predominantly used for concise data transmission from server to client. All major 
%
%Generally, there are two important aspects for data transmission: the size of the data being transferred and the time it takes to decompress that data.
%
%The first aspected is important as outwards traffic is expensive while the second aspect takes the overall response time into account.
%Zopfli~\cite{vandevenne2016zopfli} provides zlib compatible data streams with a slightly higher compression rate at the expense of a dramatically increased compression time.
%Another tool, recently published by Google, is Brotli~\cite{google2016brotli}---an LZ77-based compression tools that provides comparable compression rates to bzip2 at slow compression speed but high decompression speed.
%Finally, Facebook released Zstandard~\cite{collet2016smaller}, which claims to provide faster speeds and higher ratios.

\paragraph{General-purpose Compression.} Compression tools are well studied and compared in regards to their compression rate, compression speed, and decompression speed~\cite{alakuijala2015comparison,squash2016squash,mahoney2016large,lzturbo2016lzturbo,turbobench2016turbobench}. Classic encoders are zlib~\cite{deutsch1996zlib} (which utilizes LZ77~\cite{ziv1977universal} and Huffmann codes~\cite{huffman1952method}), LZ78~\cite{ziv1978compression}, LZF~\cite{lehmann2016liblzf}, LZO~\cite{oberhumer2005lzo}, LZW~\cite{welch1984technique}, bzip2~\cite{alakuijala2015comparison},
% (using run-length encoding, the Burrows-Wheeler transformation~\cite{burrows1994block}, move-to-front encoding, and delta encoding), 
and LZMA; LZMA is usually reported to provide very high compression rates.% which we observe also in our experiments. 
Recent efforts to optimize these approaches include Zopfli~\cite{vandevenne2016zopfli}, Brotli~\cite{google2016brotli}, and Zstandard~\cite{collet2016smaller}.

% Several efforts tried to optimize these approaches, such as Zopfli~\cite{vandevenne2016zopfli}, which provides zlib compatible data streams with a slightly higher compression rate at the expense of an increased compression time. Another optimization, recently published by Google, is Brotli~\cite{google2016brotli}---an LZ77-based compression tools that provides comparable compression rates to bzip2 with high decompression speed. Finally, Facebook released Zstandard~\cite{collet2016smaller}, which claims to provide faster speeds and higher ratios. 

All general-purpose methods are targeted towards handling any kind of data and we evaluate a variety on our datasets, resulting in compression without exploiting the typical characteristics of segmentation data. In addition, some methods are not able to handle the size of our datasets (Sec. \ref{sec:results}).

\paragraph{Image Compression.}
Many compression schemes for 2D images exist. The two most prominent lossless compression methods are PNG and JPEG2000 --- optimized for photographs. These techniques rely on frequency reduction (using e.g., wavelet or discrete cosine transform) and value prediction of images based on local context (differential pulse-code modulation). In contrast to photographs, segmentation data consists of large invariant regions and no natural relationship between their identifiers.

The typical frequency reductions and value predictions can not be applied to segmentation data. Therefore, off-the-shelf image compression techniques are not optimized and provide lower compression ratios. We still include PNG and JPEG2000 for evaluation.

\paragraph{Video Compression.} 
The \textit{z}-axis of a segmentation stack can be mapped to the time axis of video data. This relates video compression to segmentation compression. In practice, the change between slices in connectomics segmentation data is limited given natural constraints of the shape of cell bodies and restrictions in between-slice resolution. 
%There is a variety of different lossless compression tools for videos.

The currently most widespread video compression is x264 which provides lossless encoding when quantization is disabled. Intuitively, this does not yield the best performance on segmentation data, since any possible color space optimization is not applicable. 
Nevertheless, we include x264 for evaluation in Sec.~\ref{sec:results}.

\paragraph{Neuroglancer.} 
% \subsection{Neuroglancer} \label{sec:rl:neuroglancer}
The closest current compression tool is Neuroglancer~\cite{google2016compressed}---a web-based 3D segmentation viewer that provides a compression scheme for segmentation data. 
The assumption behind the compression scheme is that, on average, small local areas will contain very few distinct segment identifiers with high probability. 
Based on this assumption, Neuroglancer divides the segmentation data into several small blocks. 
Consider an arbitrary block $b$ with $N$ distinct segment identifiers.
Neuroglancer maps the identifier for each pixel into $[0, N)$ and stores a lookup table for $[0, N)$ to the original value. 
This reduces the number of bits needed to encode a pixel in a given block to $\log_2{N}$. 
The lookup table requires $64 * N$ bits of information to reconstruct the original data. 
In order to allow random access, Neuroglancer creates a header that stores the byte offset to the lookup table and encoded values for each block.
We compare the performance of Neuroglancer with our scheme in Sec. \ref{sec:results}.
