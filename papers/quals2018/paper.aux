\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{richard2016imaging}
\citation{jovanic2016competitive}
\citation{richard2016imaging}
\citation{suissa2016automatic}
\citation{haehn2017scalable}
\citation{funke2017deep}
\citation{parag2017anisotropic}
\citation{haehn2017guided}
\citation{error_correction_using_CNN}
\citation{briggman2012volume}
\citation{kaynig2015large}
\citation{seymour2016rhoananet}
\citation{ronneberger2015u}
\citation{bogovic2013learned}
\citation{ciresan2012deep}
\citation{jain2010boundary}
\citation{amelio_segmentation}
\citation{lee2015recursive}
\citation{parag2017anisotropic}
\citation{lee2017superhuman}
\citation{cciccek20163d}
\citation{turaga2010convolutional}
\citation{cciccek20163d}
\citation{briggman2009maximin}
\citation{januszewski2016flood}
\citation{zeng2017deepem3d}
\citation{lee2017superhuman}
\citation{lee2017superhuman}
\citation{andres2012globally}
\citation{zlateski2015image}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{richard2016imaging}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{jovanic2016competitive}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{richard2016imaging,suissa2016automatic}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{haehn2017scalable}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{funke2017deep,parag2017anisotropic}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{haehn2017guided,error_correction_using_CNN}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{briggman2012volume}{{1}{1}{section.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{1}{section.2}}
\@writefile{brf}{\backcite{kaynig2015large}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{seymour2016rhoananet,ronneberger2015u,bogovic2013learned,ciresan2012deep,jain2010boundary,amelio_segmentation}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{lee2015recursive,parag2017anisotropic,lee2017superhuman,cciccek20163d,turaga2010convolutional}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{cciccek20163d}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{briggman2009maximin}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{januszewski2016flood}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{zeng2017deepem3d}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{lee2017superhuman}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{lee2017superhuman}{{1}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{andres2012globally}{{1}{2}{figure.caption.1}}}
\citation{funke2017deep}
\citation{lee2017superhuman}
\citation{jain2011learning}
\citation{seymour2016rhoananet}
\citation{nunez2014graph}
\citation{10.1371/journal.pone.0125825}
\citation{parag2017anisotropic}
\citation{zlateski2015image}
\citation{bogovic2013learned}
\citation{seymour2016rhoananet}
\citation{nunez2014graph}
\citation{10.1371/journal.pone.0125825}
\citation{parag2017anisotropic}
\citation{zlateski2015image}
\citation{jain2011learning}
\citation{beier2017multicut}
\citation{haehn2017guided}
\citation{haehn2014design}
\citation{mojo2}
\citation{rolnick2017morphological}
\citation{error_correction_using_CNN}
\citation{sato2000teasar}
\citation{sato2000teasar}
\citation{zhao2014automatic}
\citation{sato2000teasar}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Our framework uses both geometric and topological biological constraints for region merging. We (a) generate a simplified graph from the skeleton representation of the input segmentation, (b) train a classifier to learn the edge weight based on the shape of the segment connection, and (c) perform graph partitioning with acyclic constraints.\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:teaser_pipeline}{{1}{2}{Our framework uses both geometric and topological biological constraints for region merging. We (a) generate a simplified graph from the skeleton representation of the input segmentation, (b) train a classifier to learn the edge weight based on the shape of the segment connection, and (c) perform graph partitioning with acyclic constraints.\relax }{figure.caption.1}{}}
\@writefile{brf}{\backcite{zlateski2015image}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{funke2017deep,lee2017superhuman}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{jain2011learning,seymour2016rhoananet,nunez2014graph,10.1371/journal.pone.0125825,parag2017anisotropic,zlateski2015image}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{bogovic2013learned}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{seymour2016rhoananet,nunez2014graph,10.1371/journal.pone.0125825,parag2017anisotropic,zlateski2015image}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{jain2011learning}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{beier2017multicut}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{haehn2017guided,haehn2014design,mojo2}{{2}{2}{figure.caption.1}}}
\@writefile{brf}{\backcite{rolnick2017morphological,error_correction_using_CNN}{{2}{2}{figure.caption.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Preliminary Methods}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Skeleton-Based Graph Generation}{2}{subsection.3.1}}
\newlabel{sec:skeletonization}{{3.1}{2}{\hskip -1em.~Skeleton-Based Graph Generation}{subsection.3.1}{}}
\@writefile{brf}{\backcite{zhao2014automatic}{{2}{3.1}{figure.caption.2}}}
\@writefile{brf}{\backcite{sato2000teasar}{{2}{3.1}{figure.caption.2}}}
\citation{chatfield2014return}
\citation{keuper2015efficient}
\citation{keuper2015efficient}
\citation{keuper2015efficient}
\citation{keuper2015efficient}
\citation{andres2011probabilistic}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example skeletons (in black) extracted from segments using a variant of the TEASER algorithm\nobreakspace  {}\cite  {sato2000teasar}. These skeletons not only capture the shape of the segmentation, but also provide endpoints useful for region merging proposals.\relax }}{3}{figure.caption.2}}
\@writefile{brf}{\backcite{sato2000teasar}{{3}{2}{figure.caption.2}}}
\newlabel{fig:skeletonization}{{2}{3}{Example skeletons (in black) extracted from segments using a variant of the TEASER algorithm~\cite {sato2000teasar}. These skeletons not only capture the shape of the segmentation, but also provide endpoints useful for region merging proposals.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Learning-based Edge Weight}{3}{subsection.3.2}}
\newlabel{sec:edge-weights}{{3.2}{3}{\hskip -1em.~Learning-based Edge Weight}{subsection.3.2}{}}
\@writefile{brf}{\backcite{chatfield2014return}{{3}{3.2}{subsection.3.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Geometric constraints for region merging. We show two region merging proposals. On the left, the segments do not belong to the same neuron, as evidenced by the sharp turning radius indicated by the arrow; while on the right, the segments should be merged due to the continuity of the 3D shape. Instead of using handcrafted geometric features, we train a convolutional neural networks to automatically learn them from the ground truth labels.\relax }}{3}{figure.caption.3}}
\newlabel{fig:turn-radii}{{3}{3}{Geometric constraints for region merging. We show two region merging proposals. On the left, the segments do not belong to the same neuron, as evidenced by the sharp turning radius indicated by the arrow; while on the right, the segments should be merged due to the continuity of the 3D shape. Instead of using handcrafted geometric features, we train a convolutional neural networks to automatically learn them from the ground truth labels.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Three erroneously split segments.\relax }}{3}{figure.caption.4}}
\newlabel{fig:merge_candidates}{{4}{3}{Three erroneously split segments.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Optimization-Based Graph Partition}{3}{subsection.3.3}}
\newlabel{sec:optimization}{{3.3}{3}{\hskip -1em.~Optimization-Based Graph Partition}{subsection.3.3}{}}
\@writefile{brf}{\backcite{keuper2015efficient}{{3}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{keuper2015efficient}{{3}{3.3}{subsection.3.3}}}
\citation{beier2017multicut}
\citation{kasthuri2015saturated}
\citation{takemura2017connectome}
\citation{zlateski2015image}
\citation{funke2017deep}
\citation{ronneberger2015u}
\citation{Turaga:2009}
\citation{10.1371/journal.pone.0125825}
\citation{takemura2017connectome}
\citation{10.1371/journal.pone.0125825}
\@writefile{brf}{\backcite{keuper2015efficient}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{keuper2015efficient,andres2011probabilistic}{{4}{3.3}{subsection.3.3}}}
\@writefile{brf}{\backcite{beier2017multicut}{{4}{3.3}{equation.3.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Experiments}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Datasets}{4}{subsection.4.1}}
\newlabel{sec:dataset}{{4.1}{4}{\hskip -1em.~Datasets}{subsection.4.1}{}}
\@writefile{brf}{\backcite{kasthuri2015saturated}{{4}{4.1}{subsection.4.1}}}
\@writefile{brf}{\backcite{takemura2017connectome}{{4}{4.1}{subsection.4.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Method Configuration}{4}{subsection.4.2}}
\@writefile{brf}{\backcite{zlateski2015image}{{4}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{funke2017deep}{{4}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{ronneberger2015u,Turaga:2009}{{4}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{10.1371/journal.pone.0125825}{{4}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{takemura2017connectome}{{4}{4.2}{subsection.4.2}}}
\@writefile{brf}{\backcite{10.1371/journal.pone.0125825}{{4}{4.2}{subsection.4.2}}}
\citation{glorot2010understanding}
\citation{meila2003comparing}
\citation{lee2017superhuman}
\citation{zeng2017deepem3d}
\newlabel{sec:network-parameters}{{4.2}{5}{\hskip -1em.~Method Configuration}{subsection.4.2}{}}
\@writefile{brf}{\backcite{glorot2010understanding}{{5}{4.2}{subsection.4.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Error Metrics}{5}{subsection.4.3}}
\newlabel{sec:variation-of-information}{{4.3}{5}{\hskip -1em.~Error Metrics}{subsection.4.3}{}}
\@writefile{brf}{\backcite{meila2003comparing}{{5}{4.3}{subsection.4.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Results}{5}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Empirical Ablation Studies}{5}{subsection.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Segmentation benchmark results on three volumes. We compare our method (red) to the baseline segmentation (green) and an oracle (blue) that optimally partitions our constructed graph from our method. Lower VI scores are better. Our method improves the segmentation accuracy over the baseline in all cases. Note that our model is only trained on the Kasthuri training volume and it generalizes well to the FlyEM dataset.\relax }}{6}{figure.caption.5}}
\newlabel{fig:variation-of-information}{{5}{6}{Segmentation benchmark results on three volumes. We compare our method (red) to the baseline segmentation (green) and an oracle (blue) that optimally partitions our constructed graph from our method. Lower VI scores are better. Our method improves the segmentation accuracy over the baseline in all cases. Note that our model is only trained on the Kasthuri training volume and it generalizes well to the FlyEM dataset.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces (left) Segments of neurons before they were correctly merged by our method. (right) Circles indicate areas of wrong merges by our method (red) or by the initial pixel-based segmentation (blue).\relax }}{6}{figure.caption.6}}
\newlabel{fig:qualitative-results}{{6}{6}{(left) Segments of neurons before they were correctly merged by our method. (right) Circles indicate areas of wrong merges by our method (red) or by the initial pixel-based segmentation (blue).\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The results of our graph pruning approach compared to the baseline graph with all adjacent regions. We show the number of true merge locations (e.g., 974) compared to total number of edges in the graph (e.g., 25,798) for each case. The number of missed splits corresponds to the number of split errors that our method misses compared to an adjacency matrix.\relax }}{6}{table.caption.7}}
\newlabel{table:skeletonization}{{1}{6}{The results of our graph pruning approach compared to the baseline graph with all adjacent regions. We show the number of true merge locations (e.g., 974) compared to total number of edges in the graph (e.g., 25,798) for each case. The number of missed splits corresponds to the number of split errors that our method misses compared to an adjacency matrix.\relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The top two examples correspond to segment pairs that we incorrectly prune from the graph. The distance between the circled endpoints is too great. The bottom two examples show pairs of segments that belong to the same neuron but are not adjacent in the input segmentation. However, we correctly merge these pairs.\relax }}{6}{figure.caption.8}}
\newlabel{fig:skeleton-results}{{7}{6}{The top two examples correspond to segment pairs that we incorrectly prune from the graph. The distance between the circled endpoints is too great. The bottom two examples show pairs of segments that belong to the same neuron but are not adjacent in the input segmentation. However, we correctly merge these pairs.\relax }{figure.caption.8}{}}
\@writefile{brf}{\backcite{lee2017superhuman,zeng2017deepem3d}{{6}{5.1}{table.caption.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The receiver operating characteristic (ROC) curves of our classifier on three connectomics datasets. The classifier works best on previously unseen data of the Kasthuri volume.\relax }}{7}{figure.caption.9}}
\newlabel{fig:receiver-operating-characteristic}{{8}{7}{The receiver operating characteristic (ROC) curves of our classifier on three connectomics datasets. The classifier works best on previously unseen data of the Kasthuri volume.\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Precision, recall, and accuracy changes between CNN only and CNN paired with graph-optimized reconstructions for the training and three test datasets. The combined method results in better precision and accuracy. The lifted multicut extension provides very slight improvements in recall and accuracy over these three datasets.\relax }}{7}{table.caption.10}}
\newlabel{table:multicut}{{2}{7}{Precision, recall, and accuracy changes between CNN only and CNN paired with graph-optimized reconstructions for the training and three test datasets. The combined method results in better precision and accuracy. The lifted multicut extension provides very slight improvements in recall and accuracy over these three datasets.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Computational Performance}{7}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Proposed Work}{7}{section.6}}
\citation{collet2016smaller}
\citation{deutsch1996zlib}
\citation{google2016brotli}
\citation{lehmann2016liblzf}
\citation{oberhumer2005lzo}
\citation{pavlov2007lzma}
\citation{seward1998bzip2}
\citation{vandevenne2016zopfli}
\citation{welch1984technique}
\citation{ziv1978compression}
\citation{roelofs1999png}
\citation{skodras2001jpeg}
\citation{aimar2005x264}
\citation{google2016compressed}
\citation{he2009fast}
\@writefile{toc}{\contentsline {section}{\numberline {7}\hskip -1em.\nobreakspace  {}Compression}{8}{section.7}}
\@writefile{brf}{\backcite{collet2016smaller,deutsch1996zlib,google2016brotli,lehmann2016liblzf,oberhumer2005lzo,pavlov2007lzma,seward1998bzip2,vandevenne2016zopfli,welch1984technique,ziv1978compression}{{8}{7}{figure.caption.11}}}
\@writefile{brf}{\backcite{roelofs1999png,skodras2001jpeg}{{8}{7}{figure.caption.11}}}
\@writefile{brf}{\backcite{aimar2005x264}{{8}{7}{figure.caption.11}}}
\@writefile{brf}{\backcite{google2016compressed}{{8}{7}{figure.caption.11}}}
\@writefile{brf}{\backcite{he2009fast}{{8}{7}{equation.7.2}}}
\bibstyle{ieee}
\bibdata{paper}
\bibcite{aimar2005x264}{1}
\bibcite{andres2012globally}{2}
\bibcite{beier2017multicut}{3}
\bibcite{bogovic2013learned}{4}
\bibcite{briggman2009maximin}{5}
\bibcite{briggman2012volume}{6}
\bibcite{cciccek20163d}{7}
\bibcite{ciresan2012deep}{8}
\bibcite{collet2016smaller}{9}
\bibcite{deutsch1996zlib}{10}
\bibcite{funke2017deep}{11}
\bibcite{google2016brotli}{12}
\bibcite{google2016compressed}{13}
\bibcite{haehn2017scalable}{14}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Compresso works by dividing up the segmentation data into small blocks. On the left we show the intersection of three segments in a $4\times 4\times 1$ window. We extract a boundary map from this segmentation to transform the window into an integral value between $0$ and $2^{16} - 1$. The location $i$ requires some additional bookkeeping. On the right are the $100$ most common $8 \times 8 \times 1$ windows accounting for $\sim 82\%$ of the volume on a representative dataset.\relax }}{9}{figure.caption.11}}
\newlabel{fig:compression}{{9}{9}{Compresso works by dividing up the segmentation data into small blocks. On the left we show the intersection of three segments in a $4\times 4\times 1$ window. We extract a boundary map from this segmentation to transform the window into an integral value between $0$ and $2^{16} - 1$. The location $i$ requires some additional bookkeeping. On the right are the $100$ most common $8 \times 8 \times 1$ windows accounting for $\sim 82\%$ of the volume on a representative dataset.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}\hskip -1em.\nobreakspace  {}Conclusions}{9}{section.8}}
\bibcite{haehn2017guided}{15}
\bibcite{haehn2014design}{16}
\bibcite{he2009fast}{17}
\bibcite{jain2010boundary}{18}
\bibcite{jain2011learning}{19}
\bibcite{januszewski2016flood}{20}
\bibcite{jovanic2016competitive}{21}
\bibcite{kasthuri2015saturated}{22}
\bibcite{kaynig2015large}{23}
\bibcite{seymour2016rhoananet}{24}
\bibcite{mojo2}{25}
\bibcite{lee2015recursive}{26}
\bibcite{lee2017superhuman}{27}
\bibcite{lehmann2016liblzf}{28}
\bibcite{nunez2014graph}{29}
\bibcite{oberhumer2005lzo}{30}
\bibcite{10.1371/journal.pone.0125825}{31}
\bibcite{parag2017anisotropic}{32}
\bibcite{pavlov2007lzma}{33}
\bibcite{roelofs1999png}{34}
\bibcite{rolnick2017morphological}{35}
\bibcite{ronneberger2015u}{36}
\bibcite{richard2016imaging}{37}
\bibcite{seward1998bzip2}{38}
\bibcite{skodras2001jpeg}{39}
\bibcite{suissa2016automatic}{40}
\bibcite{turaga2010convolutional}{41}
\bibcite{vandevenne2016zopfli}{42}
\bibcite{amelio_segmentation}{43}
\bibcite{welch1984technique}{44}
\bibcite{zeng2017deepem3d}{45}
\bibcite{ziv1978compression}{46}
\bibcite{zlateski2015image}{47}
\bibcite{error_correction_using_CNN}{48}
