% !TEX root =  paper.tex
\begin{figure}[t!]
	\centering
	\includegraphics[width=\linewidth]{./figures/teaser_v4.png}
	\caption{Our proposed framework uses both local and global biological constraints to improve an input segmentation. We generate a simplified graph from an input segmentation (a), train a classifier to learn neuron shape constraints locally (b), and perform graph partitioning to enforce domain-specific properties globally.}
	\label{fig:teaser_pipeline}
\end{figure}
\section{Method}
%\begin{figure*}[t!]
%	\begin{center}
%		\subfloat[]{\includegraphics[width=0.24\linewidth]{./figures/methods/multicut_input.png}}
%		\subfloat[]{\includegraphics[width=0.24\linewidth]{./figures/methods/pre-multicut.png}}
% 		\subfloat[]{\includegraphics[width=0.24\linewidth]{./figures/methods/multicut-graph.png}}
% 		\subfloat[]{\includegraphics[width=0.24\linewidth]{./figures/methods/post-multicut.png}}
% 	\end{center}
% 	\caption{Outline of our approach, from left to right: result of the pixel-based segmentation and agglomeration algorithm; segments of several neurons from the initial segmentation; extracted skeletonized network of those segments; improved 3D reconstruction of the selected segments after graph construction and partitioning with constraints.}
% 	\label{fig:overview}
% \end{figure*}

%There are two types of errors that can occur in connectomics segmentations: \textit{split errors} and \textit{merge errors}. 
%In a split error, two segments should have been merged into one neuronal process (Fig.~\ref{fig:improved-reconstruction}). 
%In a merge error, one segment corresponds to more than one neuron.
%Generally, it is much more difficult to correct merge errors than to correct split errors, as the space of possible split proposals grows quickly~\cite{parag2015properties}.
%Thus, most reconstruction approaches are tuned towards over-segmentation with many more split than merge errors. 
%The input of our method is the over-segmentation of EM image volumes from state-of-the-art pipelines.

Our pipeline is illustrated in Fig.~\ref{fig:teaser_pipeline}.
From the input segmentation we generate a graph $G$ with nodes $N$ and edges $E$ with weights $w_e$. 
The nodes correspond to labeled segments from the input data with edges between merge candidates.
We propose the following three steps to formulate and then partition the graph while obeying constraints from the underlying biology.
First, we only consider merging segments based on a skeletonized representation of the input segmentation (Fig.~\ref{fig:teaser_pipeline}a).
This is enables us to reduce the number of nodes in the graph based on prior knowledge of the shape of neuronal processes.
Second, we train a convolutional neural network that learns biological constraints based on the shapes of the input segmentation (Fig.~\ref{fig:teaser_pipeline}b).
This network generates probabilities that segments belong to the same neuron based only on the segmentations. 
An example of one such learned constraint is that neurons have small turning radii (Fig.~\ref{fig:turn-radii}).
Third, we partition the graph using a lifted multicut formulation with additional acyclic constraints to enforce global biological constraints (Fig.~\ref{fig:teaser_pipeline}c).
The lifted multicut solution is globally consistent which we then augment to produce a tree-structured graph (i.e., one with no cycles).

\subsection{Skeleton-Based Graph Generation}
%\subsubsection{Node Generation}
\label{sec:skeletonization}
Most region merging methods create the region graph by removing small-sized segments and linking each pair of adjacent segments, which can still lead to a large graph size due to the irregular shape of neural structures.
We here use the skeleton representation of the segmentation and reduce the graph size with the geometric constraints on the connectivity of two adjacent segments to prune edges.
% a bit trivial, as everyone is doing this. maybe no need to mention
% \subsubsection{Node Pruning}
% %The simplest node generation strategy creates one node for every unique segment label in the input volume. 
% Some labels in the volume correspond to very small structures that are likely the result of segmentation errors, typically in regions with noisy raw image data. 
% It is difficult to extract useful shape features from these segments because of their small, often random, shape. 
% We prune these nodes from the graph by removing all segments with fewer than a empirical threshold $t_{seg} = 20,000$ voxels. 
% This removes on average \TODO{XX}\% of the segments in our  datasets (Sec.~\ref{sec:dataset}), leaving us with around $\TODO{XX}$ nodes per dataset (\TODO{X nodes per micrometer}). 
% Despite the large number of segments, these regions only take up \TODO{XX}\% of the total volume on average.

% \subsubsection{Edge Pruning}

\begin{figure}[t]
	\centering
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/skeleton1.png}		
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/skeleton2.png}		
	\end{minipage}
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/skeleton3.png}		
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/skeleton4.png}		
	\end{minipage}
	\caption{Example skeletons (in black) extracted from segments using a variant of the TEASER algorithm.}
	\label{fig:skeletonization}
\end{figure}

%A typical approach for generating edges produces one between all adjacent segments. 
%Two segments $l_1$ and $l_2$ are considered adjacent if there is a pair of adjacent voxels with one labeled $l_1$ and the other labeled $l_2$.
%For example, pixel-based agglomeration methods such as mean agglomeration~\cite{lee2017superhuman} or waterz~\cite{funke2017deep} consider all pairs of adjacent segments for merging.
%However, this method produces too many edges in the graph for graph-based optimization approaches. 
Our key observation is that if two segments belong to the same neuronal process, their skeleton end points should satisfy certain geometric constraints.
To extract the skeleton from each segment, we use a variant~\cite{zhao2014automatic} of the TEASER algorithm~\cite{sato2000teasar}. 
This skeletonization algorithm repeatedly uses Dijkstra's algorithm to find the farthest voxel from a seed location. 
Since this algorithm is non-linear in the number of voxels, we downsample the datasets so that there are voxel samples every $\SI{30}{\nano\meter}$ in each dimension.
Empirically, this reduced the running time for skeletonization by $30\times$, with minimal reduction in skeleton accuracy (${\sim}5\%$ fewer branches). 
Fig.~\ref{fig:skeletonization} shows four examples of extracted skeletons (in black). 
These skeletons consist of a sequence of \textit{joints}, locations that are locally a maximum distance from the segment boundary, with line segments connecting successive joints. 
We refer to joints that have only one connected neighbor as \textit{endpoints}. 
We find that approximately 70\% of the segments that are erroneously split have nearby endpoints (Fig.~\ref{fig:merge_candidates}). 

Two segments, $s_1$ and $s_2$, receive a corresponding edge if the two following conditions hold.
First, a endpoint in either $s_1$ or $s_2$ is with $t_{low}$ nm of any voxel in the other segment.
Second, there is an endpoint in $s_1$ and an endpoint in $s_2$ that are within $t_{high}$ nm of each other.
We store the midpoints between the two endpoints as the center of the potential merge in the set $\mathbb{S}_c$. 
This algorithm produces a set of segments to consider for merging. 
Only these pairs have a corresponding edge in the constructed graph.
These parameters follow empirically from noise in the skeleton generation process. 
There are occasionally spurious branches and other artifacts and these thresholds mitigate the influence of these degenerate locations.
We provide a fuller analysis in the supplemental materials.

\begin{figure}[t]
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/constraint_error.png}
	\end{minipage}
	\hfill
	\begin{minipage}{0.45\linewidth}
		\includegraphics[width=\linewidth]{./figures/constraint_success.png}
	\end{minipage}
	\caption{Two example candidates for merging. The two segments in the left example do not belong to the same neuron as evidenced by the sharp turning radius.}
	\label{fig:turn-radii}
\end{figure}
\subsection{Learning-based Edge Weight}
\label{sec:edge-weights}
We assign an edge weight $w_e$ to each edge corresponding to the probability that two nodes belong to the same neuronal process.
We train a 3D CNN model to learn these geometric constraints for valid connection between two segments from labeled volume.
\\~\\
\noindent\textbf{Edge Probability}
To predict the probabilities that two segments belong to the same neuron, we train a feed-forward convolutional network with three \textit{VGG-style} convolution blocks~\cite{chatfield2014return} and two fully connected layers before the final sigmoid activation. 

For the input of the network, we extract a cubic region of interest (ROI) around each endpoint $e$ in $\mathbb{S}_c$ as input to the CNN. 
The CNN receives three input channels for every voxel in the ROI around segments $l_1$ and $l_2$. 
The input in all of the channels is in the range $\{-0.5, 0.5\}$. 
The first channel is $0.5$ only if the corresponding voxel has label $l_1$. 
The second channel is $0.5$ only if the corresponding voxel has label $l_2$. 
The third channel is $0.5$ if the corresponding voxel is either $l_1$ or $l_2$.
We do not use the raw EM image information to avoid the need to retrain the network on datasets that have been stained differently or imaged at different resolution. 
This reduces the need for generating costly manually-labeled ground truth. 
\\~\\
\noindent\textbf{Edge Weight} Given the probability that the nodes belong to the same neuron is $p_e$, the edge weight $w_e$ is defined as
%p_e &=\mbox{CNN}(x_1, x_2) \\
\begin{align}
w_e = \log{\frac{p_e}{1 - p_e}} + \log{\frac{1 - \beta}{\beta}},
\end{align}
where $\beta$ is a tunable parameter that encourages over- or under-segmentation. 
This particular weighting scheme allows us to optimize for a Bayesian network defined on the graph~\cite{keuper2015efficient,andres2011probabilistic}.

%The first two max pooling layers are anisotropic with pooling only in the $x$ and $y$ dimensions. 
%The output of this final pooling step is flattened into a 1D vector that is input into . 
%The final layer produces probabilities with a sigmoid activation function~\cite{funahashi1989approximate}. 
%All of the other activation functions are .

% covered in the experiment
%For training we use a stochastic gradient descent optimizer with Nesterov's accelerated gradient~\cite{nesterov1983method}. 
%We employ dropouts of $0.2$ after every pooling layer and the first dense layer, and a dropout of $0.5$ after the final dense layer to prevent overfitting. 
%We discuss all other network parameters in Sec.~\ref{sec:network-parameters}.

\begin{figure}[t]
	\centering
	\begin{minipage}{0.32\linewidth}
		\includegraphics[width=\linewidth]{./figures/split_error1.png}		
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\linewidth}
		\includegraphics[width=\linewidth]{./figures/split_error2.png}				
	\end{minipage}
	\hfill
	\begin{minipage}{0.32\linewidth}
		\includegraphics[width=\linewidth]{./figures/split_error3.png}
	\end{minipage}
	\caption{Three erroneously split segments.}
	\label{fig:merge_candidates}
\end{figure}
\subsection{Optimization-Based Graph Partition}

After graph construction, we segment the graph into a globally consistent manner while enforcing topological constraints on the output.
\\~\\
%\subsubsection{Lifted Multicut}
\noindent\textbf{Lifted Multicut}
After constructing the graph we seek to partition it into labels where every label corresponds to a neuronal process. 
We formulate this graph partitioning problem as a multicut problem.
There are two primary benefits to using a multicut formulation. 
First, the number of segments in the final graph is not predetermined but depends on the input. 
Second, this minimization produces globally consistent solutions (i.e. a boundary remains only if the two corresponding nodes belong to unique segments)~\cite{keuper2015efficient}.

We apply the algorithms of Keuper et al.~\cite{keuper2015efficient} to produce a feasible solution to the multicut problem using greedy additive edge contraction.
Following their example, we employ the generalized ``lifted'' multicut formulation.
Traditional multicut solutions only consider the probabilities that two adjacent nodes belong to the same segment. 
In the ``lifted'' extension to the problem, we can penalize non-adjacent nodes that belong to different segments. 
These penalties between non-adjacent nodes are called ``lifted'' edges. 
Ideally these ``lifted'' weights represent the probability that two nodes belong to the same neuron.
However, determining such probabilities is computationally expensive.
We approximate these probabilities by finding the maximal probable path between any two nodes using Dijkstra's algorithm~\cite{keuper2015efficient}.
This is an underestimate of the probability that two nodes belong to the same neuron since it does not consider all possible paths.
Since our graphs are sufficiently small, we can generate ``lifted'' edges between all pairs of nodes. 
\\~\\
%\subsubsection{Topological Constraints}
\noindent\textbf{Topological Constraints}
By reformulating the segmentation problem as a graph partitioning one, we can enforce some global constraints on our result based on the underlying biology.
Traditional hierarchical clustering algorithms do not rely on such constraints but consider local decisions independently.
We enforce a global constraint that neurons are tree-structured and should not contain cycles. 
The multicut problems returns a series of ``collapsed'' edges between nodes that belong to the same neuron.
We iterate over these edges in order of the probability of merge generated by our CNN. 
We ``collapse'' an edge only if it does not create a cycle in the graph.
