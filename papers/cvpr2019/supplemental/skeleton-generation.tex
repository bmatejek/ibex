% !TEX root =  supplemental.tex
\section{Skeleton Generation}

Our algorithm uses skeletons to represent the volumetric shape of each label in a volume.
Each skeleton is comprised of ``joints'' which represent voxels belonging to the skeleton.
We call joints with only one neighbor ``endpoints''.

We employ a multi-step pipeline to generate skeletons for each label in the volume.
In the first step, we anisotropically downsample each label so that the output volume as an even resolution in all three dimensions.
Next, we apply a preexisting skeletonization algorithm to reduce the label volume into skeletons that represent the 3D volumetric shape.
Finally, we upsample the skeleton joints so that they fit inside the full-resolution volume.
Particular care is taken in the final two steps to identify skeleton the joints that are endpoints.

Each of the three steps has various parameters, algorithmic strategies, and associated running times. 
We constructed a skeleton benchmark dataset to test the results of these parameters in terms of accuracy and speed.~\footnote{Our benchmark dataset is freely available at \texttt{omitted for review}.}
In the following subsections we discuss the various strategies tried for each step and present the results in Sec.~\ref{sec:skeleton-results}.
Based on this analysis we downsample the label volumes to \SI{100}{\cubic\nano\meter\per\voxel}.
We use a topological thinning strategy based on preserving isthmuses~\cite{palagyi2014sequential}.
Finally, we upsample the skeleton joints and enforce that neighbor joints in the downsampled image have a close connected path in the full-resolution volume. 

\subsection{Downsample Volumes}
	
The first step in our skeleton generation downsamples the label volumes.
Often the electron microscopy (EM) images are anisotropic with higher resolution in the $x$ and $y$ dimensions than the $z$. 
Thus, we anisotropically downsample a segment so that the resultant segment is isotropic.
At best a skeleton generation algorithm will have a linear runtime in the number of voxels in the segment since each voxel needs to be labeled as skeleton or not.
Thus, running time decreases as a function of downsample rate.
One major concern for downsampling is that information (and potentially structure) is lost.
However, neuronal morphological details are preserved at resolutions as coarse as \SI{100}{\cubic\nano\meter\per\voxel}~\cite{rolnick2017morphological}.
A second major concern with downsampling is one cannot guarantee the preservation of topology since holes can collapse if their width is smaller than the downsampling factor~\cite{kraus2001topology}.
We further address this issue in Sec.~\ref{sec:skeleton-upsample}.

When we downsample a binary label volume we use a max-pooling operation.
Thus, if we downsample by a factor of $5 \times 5 \times 2$ and a single voxel in the cube $(5x, 5y, 2z) \to (5x + 5, 5y + 5, 2z + 2)$ has the label, then the downsampled voxel $(x, y, z)$ will receive that label.
Therefore, the downsampled voxel $(x, y, z)$ can contain multiple labels.


\subsection{Skeletonization}

We test three different skeletonization algorithms.
As a baseline we use the built in \texttt{scipy:skeletonize\_3d} function that implements the medial surface/axis thinning algorithm of Lee et al.~\cite{lee1994building}.
This thinning algorithm preserves the topology and uses various post-processing techniques to reduce noise.
Second, we implement the TEASER skeletonization algorithm from Sato et al.~\cite{sato2000teasar}. 
This algorithm does not preserve topology but has shown promising results on various biomedical segmentations and variants of the strategy have been used on connectomics label volumes before~\cite{konstantin2018efficient,zhao2014automatic}.
Lastly, we implement, with the help of the author, a sequential curve-thinning algorithm based on isthmuses~\cite{palagyi2014sequential}.
This fast algorithm preserves topology and uses efficient lookup tables to reduce unnecessary computation.

\subsection{Upsample Skeletons}
\label{sec:skeleton-upsample}




\subsection{Skeleton Benchmark}




\subsection{Results}
\label{sec:skeleton-results}

Max-pool downsample

Upsample technique (closest to center)

Naive approach to upsample (with examples)

A* approach for determining connectedness

Discuss increase in running time

Differences between topological thinning and TEASER (show issues  holes in segments)

TEASER doesn't need non naive


%\section{Skeletonization}

%There are three steps in our skeleton generation pipeline: downsampling, skeletonization, and upsampling. 
%Here we explore the advantages and disadvantages of each step both qualitatively by visualing inspecting results and quantitatively by publishing a novel ``Skeleton Benchmark''. 
%Lastly, we compare three widely used skeletonization algorithms against our benchmark to determine the relative efficiencies and accuracies on segmented neuronal data.

%\subsection{Skeleton Pipeline}

%First, we downsample the original image to a resolution of $t_{res}\si{\nm^3}$ where $t_{res}$ is a tunable parameter. 
%We experiment with values of $t_{res}$ ranging from $30$ to $200$ and display the results in Sec.~\ref{skeleton-results}.
%In general, topologically preserving downsampling is impossible as holes can collapse when the width of a gap is smaller than the downsampling ratio.
%This is particularly common on dendrites where multiple spines protrude closely. 


%There are two major benefits to downsampling the data before running the skeletonization algorithm.
%First, the overall skeletonization running time greatly decreases as a function of downsample resolution.
%Two of our baseline skeleton algorithms have running times linear in the number of voxels in the input segmentation.
%The third uses Dijkstra's algorithm to find globally distance voxels to form branches of the skeleton ``tree''.
%Therefore, the computational complexity is $\mathcal{O}(n \log{n})$ since the number of edges in a structured voxel grid is $\mathcal{O}(n)$.
%Sec.~\ref{skeleton-results} further explores the running time of the skeleton algorithms as a function of downsample rate.
%Second, downsampling reduces the number of spurious endpoints.
%Our input segmentations often have noisy boundaries with small spikes protruding from the surface that do not represent larger dendritic spines.
%Downsampling the input segmentations has a smoothing effect that eliminates these small bumps while 



%The last step of the algorithm ``upsamples'' the skeleton to the resolution of the original image.
%If we downsample in $x$, $y$, and $z$ by $down_x$, $down_y$, and $down_z$ respectively, we cannot merely take a skeleton joint at $(u, v, w)$ and identify $(down_x * u, down_y * v, down_z * w)$ as the upsampled joint since this point may not fall within the original label boundary.
%It is during this process that we also identify which skeleton joints are also endpoints.


%\subsection{Skeleton Benchmark}

%We created a skeleton benchmark to evaluate the three different skeletonization approaches with varying downsampling parameters.
%We identify all of the endpoint locations


%\subsection{Skeleton Results}
%\label{skeleton-results}

%\paragraph{Endpoint Accuracy.}

%\paragraph{Running Times.}




