\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{0446} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Graph-Based Neural Reconstruction from Skeletonized 3D Networks}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}


%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW

We would like to thank the reviewers for their time.

\section*{Reviewer 1}

\textbf{A1.} We thank the reviewer for linking to these four recent papers, although they are working on a different issue. Our algorithm works by correcting the merge errors generated by current segmentation algorithms.  The Funke et al. paper is actually a pre-paper arxiv submission for the CVPR conference. We take issue with the statement that ``the paper does not adequately reflect prior art, rendering its own conclusions questionable.'' In fact, reviewer 3 wrote as a strength ``the list of references is impressive, demonstrating comprehensive knowledge of both recent methods and the application domain''.

\textbf{A2.} This claim refers to building on-top of existing segmentation pipelines. We will clarify this point in the next revision. LASH extracts a graph from superpixels generated from a watershed. We extract a graph from this output at a different abstraction layer.

\textbf{A3.} We don't agree with the assertion that these are ``drastic'' examples but rather honest mistakes. We can easily add an additional reference for U-Net that extends the original paper to 3D. In fact, citing reference [18] for the multicut algorithm is not a mistake at all. We use the graph optimization library from Bjoern Andres\footnote{http://www.andres.sc/graph.html} which cites [18] (his own work) for the multicut package. 
We will update the reference to Funke et al. as this was a simple LaTex citation mistake.

\textbf{B1.} We agree that skeletonization can improve but our goal for this paper is to use an existing method and explore its application to new ideas.

\textbf{B2.} This is incorrect, the skeletons from the figures are from the Kasthuri et al. data.

\textbf{B3.} This is addressed in the paper. First the cubic region of interest is in nanometers, not voxels. From section 4.8, ``Since our CNN only takes as input a region of the label volume we can train on anisotropic data and test on isotropic data.'' In the same section we show the results when training on isotropic data (Fig. 9). 

\textbf{B4.} We address this in the supplemental material. We can include a reference to the supplemental material in the future.

\textbf{B5.} Without the images, we avoid retraining for every new dataset that has different staining or resolution, which is every dataset that we receive from our biology collaborators. We will clarify this point in a revision.

\textbf{B6.} This analysis is provided in Sec. 4.7 and Table 2.

\textbf{B7.} We are not using ``lifted'' multicut but only the greedy-edge contraction method.

\textbf{B8. Section 3.4 states that acyclic partitionings are enforced. Since acyclic regions are not an inherent outcome of the multicut algorithm, this requires a modified multicut, but the modification is unspecified.}

\textbf{B9.} Here we reference the possibility for additional constraints using our new novel framework. Currently that enforces neurons to be acyclic but in the future can use other information. The important idea was that this framework allows domain-specific constraints.

\textbf{C1.} We use NeuroProof because it scales to terabyte datasets. Flood filling networks provide better accuracy but are 25x slower and work on a single neuron at a time. For dense reconstruction, NeuroProof is state-of-the-art. Variation of Information overcomes previous limitations of rand error and is more widely used by the community~\cite{lee2017superhuman,nunez2013machine}. The SNEMI3D dataset is considered outdated as concluded in~\cite{lee2017superhuman}. 

\textbf{C2.} Most of the current state-of-the-art methods are too slow to run on very large datasets.

\section*{Reviewer 2}

Unfortunately the challenge datasets (SNEMI3D, CREMI) are too small for meaningful improvement. The datasets presented in our paper are roughly ten times the size of these challenge datasets. 

\section*{Reviewer 3}

\textbf{Address acyclic nature of graph.}

There are some polynomial time algorithms for finding the general minimum spanning forest on a general graph ($O(V^2 E)$). The multicut algorithm we use has a running time of $O(V^2 log(V))$, although provides an approximation to the optimal solution. Exploring both of these algorithms is reasonable in a future revision. 

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
