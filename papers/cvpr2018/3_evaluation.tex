% !TEX root =  paper.tex
\section{Experimental Results}

We evaluate our method by comparing it to a state-of-the-art pixel-based reconstruction approach using datasets from two different species.

\subsection{Datasets}
\label{sec:dataset}

\paragraph{Kasthuri.}
The Kasthuri dataset consists of scanning electron microscope images of the neocortex of a mouse brain~\cite{kasthuri2015saturated}. This dataset is $5342 \times 3618 \times 338$ voxels in size. The resolution of the dataset is $\SI[product-units=single]{3 x 3 x 30}{\nano\meter}^3$ per voxel. We evaluate our methods using the left cylinder of this 3-cylinder dataset. We downsample the dataset in the $x$ and $y$ dimensions to give a final resolution of $\SI[product-units=single]{6 x 6 x 30}{\nano\meter}^3$ per voxel. We divide the dataset into two volumes (Vol. 1 and Vol. 2) along the $x$ dimension, where each volume is $\SI[product-units=single]{8.0 x 10.9 x 10.1}{\micro\meter}^3$ or $1335 \times 1809 \times 338$ voxels.

\paragraph{FlyEM.}
The FlyEM dataset comes from the mushroom body of a 5-day old adult male \textit{Drosophila} fly imaged by a focused ion-beam milling scanning electron microscopy~\cite{takemura2017connectome}. The mushroom body in this species is the major site of associative learning. The original dataset contains a $\SI[product-units=single]{40 x 50 x 120}{\micro\meter}^3$ volume with a resolution of $\SI[product-units=single]{10 x 10 x 10}{\nano\meter}^3$ per voxel. We use two cubes (Vol. 1 and Vol. 2) of size $\SI[product-units=single]{10 x 10 x 10}{\micro\meter}^3$ or $1000 \times 1000 \times 1000$ voxels.

\subsection{Pixel-Based Segmentations}
\label{sec:neuroproof}

The segmentation of the Kasthuri dataset was computed by agglomerating 3D supervoxels produced by the z-watershed algorithm from 3D affinity predictions~\cite{zlateski2015image}. A recent study by Funke et al.~\cite{schlegel2017learning} demonstrated superior performance of such methods over existing ones on anisotropic data. We learn 3D affinities using MALIS loss with a U-net~\cite{ronneberger2015u,Turaga:2009}. We apply the z-watershed algorithm with suitable parameters to compute a 3D oversegmentation of the volume. The resulting 3D oversegmentation is then agglomerated using the technique of context-aware delayed agglomeration to generate the final segmentation~\cite{10.1371/journal.pone.0125825}.

For the FlyEM data, based on the authors' suggestion~\cite{takemura2017connectome}, we applied a context-aware delayed agglomeration algorithm~\cite{10.1371/journal.pone.0125825} that shows improved performance on this dataset over the pipeline used in the original publication. This segmentation framework learns voxel and supervoxel classifiers with an emphasis to minimize under-segmentation error. At the same time this framework produces lower over-segmentation than standard algorithms. The algorithm first computes multi-channel 3-D predictions for membranes, cell interiors, and mitochondria, among other cell features. The membrane prediction channel is used to produce an over-segmented volume using 3D watershed, which is then agglomerated hierarchically up to a certain confidence threshold. We used exactly the same parameters as the publicly available code for this algorithm.

\subsection{Graph Pruning Parameters}

The two parameters for the graph pruning algorithm (Sec.~\ref{sec:skeletonization}) are $t_{low}$ and $t_{high}$. Ideally, the merge candidates output by this algorithm will contain all possible positive examples with a very limited number of negative examples. After considering various thresholds, we find that $t_{low} = \SI{240}{\nano\meter}$ and $t_{high} = \SI{600}{\nano\meter}$ produce the best results considering this objective.

In our implementation we use nanometers for these thresholds and not voxels. Connectomics datasets often have lower sample resolutions in $z$ because of limitations during sample preparation. Using nanometers allows us to have uniform units across all of these datasets and calculate the thresholds in voxels at runtime. For example, the thresholds in voxels are $t_{low} = (40, 40, 8)$ and $t_{high} = (100, 100, 20)$ for the anisotropic Kasthuri dataset and $t_{low} = (24, 24, 24)$ and $t_{high} = (60, 60, 60)$ for the isotropic FlyEM dataset.

%Table \ref{table:skeletonization} shows the overall success of our graph creation method with candidate pruning. \hp{not clear what this table is showing--explain. put this in the experiment section where you discuss the results}

\subsection{Classifier Training}
\label{sec:network-parameters}

We use the left cylinder of the Kasthuri dataset for training and validation. We train on 80\% of the potential merge candidates for this volume.
We validate the CNN classifier on the remaining 20\% of candidates. We apply data augmentation to the generated examples to increase the size of the training datasets. We consider all rotations of $90$ degrees along the $xy$-plane in addition to mirroring along the $x$ and $z$ axes. This produces 16 times more training data.

We consider networks with varying input sizes, optimizers, loss functions, filter sizes, learning rates, and activation functions. The supplemental material includes information on the experiments that determined these final parameters. Table~\ref{table:architecture} provides the parameters of the final network. There are 7,294,705 learnable parameters in our final architecture. All the parameters are randomly initialized following the Xavier uniform distribution~\cite{glorot2010understanding}. Training concluded after 34 epochs.

\begin{table}[h!]
	\centering
	\begin{tabular}{l l} \hline
		\textbf{Parameters} & \textbf{Values} \\ \hline
		Loss Function & Mean Squared Error \\
		Optimizer & SGD  with Nesterov Momentum \\
		Momentum & 0.9 \\
		Initial Learning Rate & 0.01 \\
		Decay Rate & $5 * 10^{-8}$ \\
		Activation & LeakyReLU $(\alpha = 0.001)$ \\
		Kernel Sizes & $3 \times 3 \times 3$ \\
		Filter Sizes & $16 \to 32 \to 64$ \\ \hline
	\end{tabular}
	\caption{Training parameters.}
	\label{table:architecture}
\end{table}




%\subsection{Graph-based Strategies}

%Applying a top-down globally optimal partitioning function allows us to enforce some domain-specific constraints on the reconstruction.
%In connectomics, we want to enforce the topological restrictions that each partition in the graph has a genus of zero.
%The greedy-additive heuristic solves the multicut graph partitioning problem by enforcing this constraint.
%Previous agglomeration strategies at the per-region level consider neighboring superpixels in order of merge probability.
%These superpixels are clustered hierarchically without concern for global constraints.
%We compare the benefits of a top-down partitioning function versus the traditional bottom-up clustering methods.
