\section{Related Work}

A large amount of connectomics research considers the problem of extracting segmentation information at the voxel level from the raw EM images. 
Some methods apply computationally expensive graph partitioning techniques with a node per voxel~\cite{andres2012globally} following the previous work in computer vision using normalized cuts for image segmentation~\cite{kappes2016higher,shi2000normalized,tatiraju2008image}. 
Meanwhile, others focus on training 2-D convolutional neural networks to predict membrane probabilities per image slice~\cite{ciresan2012deep}. 
More recent strategies augment these neural networks with the $z$-dimension creating full 3-D convolutional networks~\cite{lee2015recursive}
Oftentimes these networks now produce probabilities for the affinity between voxels rather than probabilities for a voxel belonging to a cell membrane~\cite{ronneberger2015u}. 
An extensive amount of research in computer vision, applied mathematics, and statistics evaluates the loss functions and optimizers for these networks~\cite{chatfield2014return,maas2013rectifier,nesterov1983method}.

These above neural networks generate probabilities that neighboring voxels belong to the same neuron.
Many algorithms work at a level above these networks and train random-forest classifiers to produce segmentations of the EM images where every unique neuron in the volume has a unique label~\cite{seymour2016rhoananet,nunez2014graph,parag2017anisotropic,zlateski2015image} (CITE NEUROPROOF). 



Despite the successes of these algorithms they often make early local mistakes leading to errors in the segmentation. Proofreading methods create a "human-in-the-loop" framework that allows users to find errors and correct them \cite{haehn2017scalable,haehn2017guided,haehn2014design}. Flood-filling networks merge these two steps by training an end-to-end network that takes as input raw image data and produces a segmentation \cite{januszewski2016flood}. Although these networks have produced impressed segmentation accuracies, they are currently too slow for large scale connectomics reconstructions. 

ADD SECTION ON CORRECTING SPLIT ERRORS

The fields of computer graphics, mathematics, and biomedical visualization have produced extensive research into the skeletonization of 3D binary volumes. A large amount of research has explored topologically consistent thinning algorithms and potential medical applications of these methods \cite{palagyi20003d,palagyi2001sequential}. In computer graphics, fast medial axis algorithms allow animators to quickly move characters through successive frames \cite{baran2007automatic,bharaj2012automatically}. The Tree-structure Extraction Algorithm for Accurate and Robust Skeletons (TEASER) has been used by connectomics researchers to parameterize 3-D shapes \cite{sato2000teasar,zhao2014automatic}. 

ADD SECTION ON LEARNED SHAPE FEATURES

ADD SECTION ON GRAPH PARTITIONING