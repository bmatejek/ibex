\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hildebrand2017whole}
\citation{kasthuri2015saturated}
\citation{seymour2016rhoananet}
\citation{nunez2014graph}
\citation{parag2017anisotropic}
\citation{zlateski2015image}
\citation{lee2015recursive}
\citation{ronneberger2015u}
\citation{nunez2014graph}
\citation{bogovic2013learned}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{hildebrand2017whole}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{kasthuri2015saturated}{{1}{1}{section.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces We use 3D skeletons to combine neuron labels to full reconstructions. (TODO: Add schematic figure showing the difference between voxel-based / super-voxel based / graph-based algorithms with connections and hierarchy)\relax }}{1}{figure.caption.1}}
\@writefile{brf}{\backcite{seymour2016rhoananet,nunez2014graph,parag2017anisotropic,zlateski2015image}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{lee2015recursive,ronneberger2015u}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{nunez2014graph}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{bogovic2013learned}{{1}{1}{figure.caption.1}}}
\citation{ciresan2012deep}
\citation{jain2010boundary}
\citation{amelio_segmentation}
\citation{rhoananet}
\citation{kaynig2015large}
\citation{lee2015recursive}
\citation{ronneberger2015u}
\citation{osada2002shape}
\citation{conners1984segmentation}
\citation{nunez2014graph}
\citation{ren2003learning}
\citation{bogovic2013learned}
\citation{chatfield2014return}
\citation{maas2013rectifier}
\citation{nesterov1983method}
\citation{seymour2016rhoananet}
\citation{nunez2014graph}
\citation{parag2017anisotropic}
\citation{zlateski2015image}
\citation{haehn2017scalable}
\citation{haehn2017guided}
\citation{haehn2014design}
\citation{error_correction_using_CNN}
\citation{januszewski2016flood}
\citation{andres2012globally}
\citation{kappes2016higher}
\citation{shi2000normalized}
\citation{tatiraju2008image}
\citation{horvnakova2017analysis}
\citation{kernighan1970efficient}
\citation{keuper2015efficient}
\citation{palagyi20003d}
\citation{palagyi2001sequential}
\citation{baran2007automatic}
\citation{bharaj2012automatically}
\citation{sato2000teasar}
\citation{zhao2014automatic}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {paragraph}{Voxel-based methods.}{2}{section*.3}}
\@writefile{brf}{\backcite{ciresan2012deep,jain2010boundary,amelio_segmentation,rhoananet,kaynig2015large}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{lee2015recursive}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{ronneberger2015u}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{osada2002shape}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{conners1984segmentation}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{nunez2014graph,ren2003learning}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{bogovic2013learned}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{chatfield2014return,maas2013rectifier,nesterov1983method}{{2}{2}{section*.3}}}
\@writefile{toc}{\contentsline {paragraph}{Super-voxel methods.}{2}{section*.4}}
\@writefile{brf}{\backcite{seymour2016rhoananet,nunez2014graph,parag2017anisotropic,zlateski2015image}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{haehn2017scalable,haehn2017guided,haehn2014design,error_correction_using_CNN}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{januszewski2016flood}{{2}{2}{section*.4}}}
\@writefile{toc}{\contentsline {paragraph}{Graph-based methods.}{2}{section*.5}}
\@writefile{brf}{\backcite{andres2012globally}{{2}{2}{section*.5}}}
\@writefile{brf}{\backcite{kappes2016higher,shi2000normalized,tatiraju2008image}{{2}{2}{section*.5}}}
\@writefile{brf}{\backcite{horvnakova2017analysis,kernighan1970efficient,keuper2015efficient}{{2}{2}{section*.5}}}
\@writefile{toc}{\contentsline {paragraph}{Skeletonization methods.}{2}{section*.6}}
\@writefile{brf}{\backcite{palagyi20003d,palagyi2001sequential}{{2}{2}{section*.6}}}
\@writefile{brf}{\backcite{baran2007automatic,bharaj2012automatically}{{2}{2}{section*.6}}}
\@writefile{brf}{\backcite{sato2000teasar,zhao2014automatic}{{2}{2}{section*.6}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{2}{section.3}}
\citation{sato2000teasar}
\citation{zhao2014automatic}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of the existing methods. On the left is the original EM image data. A neural network (here VD2D3D) predicts the affinities between neighboring voxels. The Zwatershed algorithm clusters these voxels into supervoxels. NeuroProof agglomerates these supervoxels to create larger segments.\relax }}{3}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:pipeline}{{2}{3}{An overview of the existing methods. On the left is the original EM image data. A neural network (here VD2D3D) predicts the affinities between neighboring voxels. The Zwatershed algorithm clusters these voxels into supervoxels. NeuroProof agglomerates these supervoxels to create larger segments.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Graph Creation}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Node Generation}{3}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Edge Generation}{3}{subsubsection.3.1.2}}
\@writefile{brf}{\backcite{sato2000teasar,zhao2014automatic}{{3}{3.1.2}{figure.caption.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Two example outputs of the TEASER skeletonization algorithm.\relax }}{3}{figure.caption.7}}
\newlabel{fig:skeletonization}{{3}{3}{Two example outputs of the TEASER skeletonization algorithm.\relax }{figure.caption.7}{}}
\citation{chatfield2014return}
\citation{funahashi1989approximate}
\citation{maas2013rectifier}
\citation{nesterov1983method}
\citation{kernighan1970efficient}
\citation{kasthuri2015saturated}
\citation{takemura2017connectome}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }}{4}{figure.caption.8}}
\newlabel{fig:merge_candidates}{{4}{4}{Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Edge Probabilities}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Network Architecture}{4}{subsubsection.3.2.1}}
\@writefile{brf}{\backcite{chatfield2014return}{{4}{3.2.1}{subsubsection.3.2.1}}}
\@writefile{brf}{\backcite{funahashi1989approximate}{{4}{3.2.1}{subsubsection.3.2.1}}}
\@writefile{brf}{\backcite{maas2013rectifier}{{4}{3.2.1}{subsubsection.3.2.1}}}
\@writefile{brf}{\backcite{nesterov1983method}{{4}{3.2.1}{subsubsection.3.2.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Agglomeration}{4}{subsection.3.3}}
\@writefile{brf}{\backcite{kernighan1970efficient}{{4}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Evaluation}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Datasets}{4}{subsection.4.1}}
\newlabel{sec:dataset}{{4.1}{4}{\hskip -1em.~Datasets}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Kasthuri}{4}{subsubsection.4.1.1}}
\@writefile{brf}{\backcite{kasthuri2015saturated}{{4}{4.1.1}{subsubsection.4.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The architecture for the neural networks follows the \textit  {VGG} style of double convolutions followed by a max pooling operation. The number of filters doubles each layer leading to a fully connected layer and a sigmoid activation function.\relax }}{5}{figure.caption.9}}
\newlabel{fig:architecture}{{5}{5}{The architecture for the neural networks follows the \textit {VGG} style of double convolutions followed by a max pooling operation. The number of filters doubles each layer leading to a fully connected layer and a sigmoid activation function.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}FlyEM}{5}{subsubsection.4.1.2}}
\@writefile{brf}{\backcite{takemura2017connectome}{{5}{4.1.2}{subsubsection.4.1.2}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Segmentation Pipeline and Baseline}{5}{subsubsection.4.1.3}}
\newlabel{sec:neuroproof}{{4.1.3}{5}{Segmentation Pipeline and Baseline}{subsubsection.4.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Skeleton Pruning}{5}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Classifier Training}{6}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The parameters for the trained neural network.\relax }}{6}{table.caption.10}}
\newlabel{table:architecture}{{1}{6}{The parameters for the trained neural network.\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {paragraph}{Data Augmentation.}{6}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Graph-based Strategies}{6}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}\hskip -1em.\nobreakspace  {}Variation of Information}{6}{subsection.4.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces An example of the benefits of using multicut over simpler agglomeration schemes. If we input this example into a na\"ive agglomeration strategy that collapses all edges with a threshold greater than 0.5, this entire segment will merge together despite the fact that nodes 2 and 4 and nodes 3 and 5 have low affinities.\relax }}{6}{figure.caption.12}}
\newlabel{fig:multicut}{{6}{6}{An example of the benefits of using multicut over simpler agglomeration schemes. If we input this example into a na\"ive agglomeration strategy that collapses all edges with a threshold greater than 0.5, this entire segment will merge together despite the fact that nodes 2 and 4 and nodes 3 and 5 have low affinities.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Results}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Skeleton Pruning}{6}{subsection.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Finding Positive Examples.}{6}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{Pruning Negative Examples.}{6}{section*.14}}
\bibstyle{ieee}
\bibdata{paper}
\bibcite{andres2012globally}{1}
\bibcite{baran2007automatic}{2}
\bibcite{bharaj2012automatically}{3}
\bibcite{bogovic2013learned}{4}
\bibcite{chatfield2014return}{5}
\bibcite{ciresan2012deep}{6}
\bibcite{conners1984segmentation}{7}
\bibcite{funahashi1989approximate}{8}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }}{7}{figure.caption.15}}
\newlabel{fig:skeleton-results}{{7}{7}{Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Precision and recall for the training and three test datasets.\relax }}{7}{table.caption.16}}
\newlabel{fig:classification}{{2}{7}{Precision and recall for the training and three test datasets.\relax }{table.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Classification Performance}{7}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Graph Based Strategies}{7}{subsection.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The receiver operating characteristic (ROC) curve for all four datasets.\relax }}{7}{figure.caption.17}}
\newlabel{fig:network-results}{{8}{7}{The receiver operating characteristic (ROC) curve for all four datasets.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}\hskip -1em.\nobreakspace  {}Variation of Information Improvements}{7}{subsection.5.4}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusions}{7}{section.6}}
\bibcite{haehn2017scalable}{9}
\bibcite{haehn2017guided}{10}
\bibcite{haehn2014design}{11}
\bibcite{hildebrand2017whole}{12}
\bibcite{horvnakova2017analysis}{13}
\bibcite{jain2010boundary}{14}
\bibcite{januszewski2016flood}{15}
\bibcite{kappes2016higher}{16}
\bibcite{kasthuri2015saturated}{17}
\bibcite{kaynig2015large}{18}
\bibcite{kernighan1970efficient}{19}
\bibcite{keuper2015efficient}{20}
\bibcite{rhoananet}{21}
\bibcite{seymour2016rhoananet}{22}
\bibcite{lee2015recursive}{23}
\bibcite{maas2013rectifier}{24}
\bibcite{nesterov1983method}{25}
\bibcite{nunez2014graph}{26}
\bibcite{osada2002shape}{27}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The improvement on variation of information from the baseline NeuroProof segmentation (green). Results closer to the origin are better.\relax }}{8}{figure.caption.18}}
\newlabel{fig:variation-of-information}{{9}{8}{The improvement on variation of information from the baseline NeuroProof segmentation (green). Results closer to the origin are better.\relax }{figure.caption.18}{}}
\bibcite{palagyi20003d}{28}
\bibcite{palagyi2001sequential}{29}
\bibcite{parag2017anisotropic}{30}
\bibcite{ren2003learning}{31}
\bibcite{ronneberger2015u}{32}
\bibcite{sato2000teasar}{33}
\bibcite{shi2000normalized}{34}
\bibcite{takemura2017connectome}{35}
\bibcite{tatiraju2008image}{36}
\bibcite{amelio_segmentation}{37}
\bibcite{zhao2014automatic}{38}
\bibcite{zlateski2015image}{39}
\bibcite{error_correction_using_CNN}{40}
