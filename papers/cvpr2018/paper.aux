\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hildebrand2017whole}
\citation{kasthuri2015saturated}
\citation{lee2015recursive}
\citation{ronneberger2015u}
\citation{nunez2014graph}
\citation{bogovic2013learned}
\citation{andres2012globally}
\citation{kappes2016higher}
\citation{shi2000normalized}
\citation{tatiraju2008image}
\citation{ciresan2012deep}
\citation{lee2015recursive}
\citation{ronneberger2015u}
\citation{chatfield2014return}
\citation{maas2013rectifier}
\citation{nesterov1983method}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{hildebrand2017whole}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{kasthuri2015saturated}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{lee2015recursive,ronneberger2015u}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{nunez2014graph}{{1}{1}{figure.caption.1}}}
\@writefile{brf}{\backcite{bogovic2013learned}{{1}{1}{figure.caption.1}}}
\citation{seymour2016rhoananet}
\citation{nunez2014graph}
\citation{parag2017anisotropic}
\citation{zlateski2015image}
\citation{haehn2017scalable}
\citation{haehn2017guided}
\citation{haehn2014design}
\citation{januszewski2016flood}
\citation{palagyi20003d}
\citation{palagyi2001sequential}
\citation{baran2007automatic}
\citation{bharaj2012automatically}
\citation{sato2000teasar}
\citation{zhao2014automatic}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}}
\@writefile{brf}{\backcite{andres2012globally}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{kappes2016higher,shi2000normalized,tatiraju2008image}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ciresan2012deep}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{lee2015recursive}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{ronneberger2015u}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{chatfield2014return,maas2013rectifier,nesterov1983method}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{seymour2016rhoananet,nunez2014graph,parag2017anisotropic,zlateski2015image}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{haehn2017scalable,haehn2017guided,haehn2014design}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{januszewski2016flood}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{palagyi20003d,palagyi2001sequential}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{baran2007automatic,bharaj2012automatically}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{sato2000teasar,zhao2014automatic}{{2}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Method}{2}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:merge_candidates}{{1}{2}{Two erroneously split segments that should merge together. Most segments that we want to merge have the same general structure.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Graph Creation}{2}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Node Generation}{2}{subsubsection.3.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Edge Generation}{2}{subsubsection.3.1.2}}
\citation{sato2000teasar}
\citation{zhao2014automatic}
\citation{chatfield2014return}
\citation{funahashi1989approximate}
\citation{maas2013rectifier}
\citation{nesterov1983method}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Two example outputs of the TEASER skeletonization algorithm.\relax }}{3}{figure.caption.3}}
\newlabel{fig:skeletonization}{{2}{3}{Two example outputs of the TEASER skeletonization algorithm.\relax }{figure.caption.3}{}}
\@writefile{brf}{\backcite{sato2000teasar}{{3}{3.1.2}{figure.caption.2}}}
\@writefile{brf}{\backcite{zhao2014automatic}{{3}{3.1.2}{figure.caption.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Edge Probabilities}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Network Architecture}{3}{subsubsection.3.2.1}}
\@writefile{brf}{\backcite{chatfield2014return}{{3}{3.2.1}{figure.caption.4}}}
\citation{kernighan1970efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The white outlines show three different possible cubic sizes to input into the neural network. The left example ($800\textrm  {nm}$) provides less local context than the middle example ($1200\textrm  {nm}$). The right example ($1600\textrm  {nm}$) extracts too large of a local region that produces noise as one of the segments leaves the bounding box only to reenter.\relax }}{4}{figure.caption.4}}
\newlabel{fig:network-radius}{{3}{4}{The white outlines show three different possible cubic sizes to input into the neural network. The left example ($800\textrm {nm}$) provides less local context than the middle example ($1200\textrm {nm}$). The right example ($1600\textrm {nm}$) extracts too large of a local region that produces noise as one of the segments leaves the bounding box only to reenter.\relax }{figure.caption.4}{}}
\@writefile{brf}{\backcite{funahashi1989approximate,maas2013rectifier}{{4}{3.2.1}{figure.caption.4}}}
\@writefile{brf}{\backcite{nesterov1983method}{{4}{3.2.1}{figure.caption.4}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Data Augmentation}{4}{subsubsection.3.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Agglomeration}{4}{subsection.3.3}}
\@writefile{brf}{\backcite{kernighan1970efficient}{{4}{3.3}{subsection.3.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Evaluation}{4}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}\hskip -1em.\nobreakspace  {}Datasets}{4}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}NeuroProof Pipeline}{4}{subsubsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}\hskip -1em.\nobreakspace  {}Preprocessing}{4}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The architecture for the neural networks follows the \textit  {VGG} style of double convolutions followed by a max pooling operation. The number of filters doubles each layer leading to a fully connected layer and a sigmoid activation function.\relax }}{5}{figure.caption.5}}
\newlabel{fig:architecture}{{4}{5}{The architecture for the neural networks follows the \textit {VGG} style of double convolutions followed by a max pooling operation. The number of filters doubles each layer leading to a fully connected layer and a sigmoid activation function.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}\hskip -1em.\nobreakspace  {}Classifier Training}{5}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}\hskip -1em.\nobreakspace  {}Graph Optimization}{5}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Results}{5}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Skeleton Pruning}{5}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Finding Merge Candidates}{5}{subsubsection.5.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Finding Split Candidates}{5}{subsubsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}\hskip -1em.\nobreakspace  {}Neural Network Classification}{5}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Merge Network}{5}{subsubsection.5.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Split Network}{5}{subsubsection.5.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}\hskip -1em.\nobreakspace  {}Graph-based Error Correction}{5}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Conclusions}{5}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The training and validation loss when training the neural network on the L. Cylinder data set.\relax }}{5}{figure.caption.6}}
\newlabel{fig:training-curve}{{5}{5}{The training and validation loss when training the neural network on the L. Cylinder data set.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The ROC curve for the training and validation datasets.\relax }}{5}{figure.caption.7}}
\newlabel{fig:roc-curve}{{6}{5}{The ROC curve for the training and validation datasets.\relax }{figure.caption.7}{}}
\bibstyle{ieee}
\bibdata{paper}
\bibcite{andres2012globally}{1}
\bibcite{baran2007automatic}{2}
\bibcite{bharaj2012automatically}{3}
\bibcite{bogovic2013learned}{4}
\bibcite{chatfield2014return}{5}
\bibcite{ciresan2012deep}{6}
\bibcite{funahashi1989approximate}{7}
\bibcite{haehn2017scalable}{8}
\bibcite{haehn2017guided}{9}
\bibcite{haehn2014design}{10}
\bibcite{hildebrand2017whole}{11}
\bibcite{januszewski2016flood}{12}
\bibcite{kappes2016higher}{13}
\bibcite{kasthuri2015saturated}{14}
\bibcite{kernighan1970efficient}{15}
\bibcite{seymour2016rhoananet}{16}
\bibcite{lee2015recursive}{17}
\bibcite{maas2013rectifier}{18}
\bibcite{nesterov1983method}{19}
\bibcite{nunez2014graph}{20}
\bibcite{palagyi20003d}{21}
\bibcite{palagyi2001sequential}{22}
\bibcite{parag2017anisotropic}{23}
\bibcite{ronneberger2015u}{24}
\bibcite{sato2000teasar}{25}
\bibcite{shi2000normalized}{26}
\bibcite{tatiraju2008image}{27}
\bibcite{zhao2014automatic}{28}
\bibcite{zlateski2015image}{29}
