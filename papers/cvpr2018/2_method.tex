\section{Method}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.92\linewidth]{./figures/skeleton1.png}
	\includegraphics[width=0.92\linewidth]{./figures/skeleton2.png}
	\caption{Skeletons using the TEASER algorithm.}
	\label{fig:skeletonization}
\end{figure}

There are two types of errors that can occur in connectomics segmentation.
The first, called a split error, occurs when there are two segments that should have been merged. The second, called a merge error, happens when one segment should be split into two. Generally, it is much more difficult to correct merge errors than to correct split errors~\cite{parag2015properties}, which is why most reconstruction approaches are tuned towards over-segmentation with many more split than merge errors. Our method takes as input over-segmentations of EM image volumes generated by state-of-the-art connectomics reconstruction pieplines (Sec.~\ref{sec:neuroproof}). Our goal is to identify locations of split errors and merge the corresponding segments automatically.

From the input segmentation we generate a graph $G$ with nodes $N$ and edges $E$ with weights $w_e$. The nodes correspond to label segments from the segmentation with edges between segments considered for merging. Ideally, our graph has edges corresponding to all of the segments which were erroneously split with few edges between correctly split segments. 
We generate a skeleton for every segment based on the intuition that a skeleton represents a simplified representation of the overall shape of the neuron. 
From this skeleton we quickly identify potential merge locations and produce the corresponding edges for the graph. 
We train a CNN to  generate the edge weights by learning a merge function from ground truth data. A multicut heuristic generates a partition on the graph where nodes in the same partition are assigned the same output label. We will now discuss the three major components to our framework (skeletonization, CNN, and graph-partition) in more detail. \hp{stopped here}

\subsection{Graph Creation}
\label{sec:skeletonization}
We generate nodes $N$ and edges $E$ to apply a graph-based optimization strategy for segmentation.
In addition, these edges receive non-negative weights.

\subsubsection{Node Generation}

The simplest node generation strategy creates one node for every unique segment label in the input volume.
However, some of the millions of labels in the volume correspond to very small volumes.
Usually these locations have noisy image data so the pixel-based methods could not provide enough information to generate a larger segment.
It is difficult to extract useful shape features from these segments because of their small, and often random, shape.
We prune these nodes from the graph by removing all segments with fewer than $20,000$ voxels.
This removes XX\% of segments on a typical connectomics dataset.

\subsubsection{Edge Generation}

A na\"ive approach to generating edges produces an edge between all adjacent segments.
Two segments $l_1$ and $l_2$ are considered adjacent if there is a pair of adjacent voxels where one has label $l_1$ and the other has label $l_2$.
NeuroProof and GALA consider all pairs of adjacent segments for merging.
For us, this method produces too many edges in the graph.
Therefore we present the following algorithm to identify pairs of segments to consider for merging.

First, we extract a skeleton from each segment using the TEASER algorithm~\cite{sato2000teasar,zhao2014automatic}.
Figure~\ref{fig:skeletonization} shows the skeletons in white of two segments from the label volume.
These skeletons consist of a sequence of \textit{joints}, locations that are a local maximum distance from the segment boundary, with line segments connecting successive joints.
We prune the joints that are within $50$ voxels of each other to reduce unnecessary branching.
For the purposes of our algorithm, joints that have only one connected neighbor are referred to as \textit{endpoints}.
Many of the segments that are erroneously split follow a similar pattern (Figure~\ref{fig:merge_candidates}).
In these split instances the two skeletons have nearby endpoints.

After skeletonization, we begin to identify segments for merge consideration with the following two-pass heuristic.
In the first pass, we iterate over all endpoints $e$ belonging to a segment $S$ and create a set of segments $\mathbb{S}_e^\prime$ that includes all labels that have a single voxel within $t_{low}$ voxels from $e$.
Elements of these sets are candidates for merging.
However the first pass often has too many candidates that should remain split so we apply an additional pass for further pruning.
In the second pass, we consider all of the segments $\mathbb{S}_e^\prime$ for every endpoint $e$.
If a segment $S^\prime \in \mathbb{S}_e^\prime$ has an endpoint within $t_{high}$ voxels of $e$, the segment $S$ and $S^\prime$ are considered for merging.
We store the midpoint between the two endpoints as the ``center" of the potential merge.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.92\linewidth]{./figures/split_error1.png}
	\includegraphics[width=0.92\linewidth]{./figures/split_error2.png}
	\caption{Two erroneously split segments.}
	\label{fig:merge_candidates}
\end{figure}


\subsection{Edge Probabilities}

The previous section outlines how to generate edges between nodes in our graph structure.
Here we introduce a neural network architecture for generating probabilities that two nodes sharing an edge belong to the same neuron.
The neural network takes as input only data from the input segmentation.

\subsubsection{Network Architecture}

Our graph generation algorithm produces 3-D locations that require further consideration for merging.
To determine which of these segments should actually merge, we train a 3-D CNN using the oversegmentation and the corresponding manually labeled ground truth data (Section~\ref{sec:dataset}).
We extract a cubic region of interest around these locations as input to the CNN.
These regions of interest provide the local information for the neural network to predict which neighboring segments belong to the same neuron.

The networks receives three input channels for every voxel in the region of interest around segments $l_1$ and $l_2$.
The input to all of the channels is in the set $\{-0.5, 0.5\}$.
The first channel is $0.5$ only if the corresponding voxel has label $l_1$.
The second channel is $0.5$ only if the corresponding voxel has label $l_2$.
The third channel is $0.5$ if the corresponding voxel is either $l_1$ or $l_2$.

Figure~\ref{fig:architecture} provides an overview of our architecture.
Our network architecture has three layers of double convolutions followed by a max pooling step following the work of Chatfield et al. that found this technique improves on single convolution layers~\cite{chatfield2014return}. The first max pooling layer is anisotropic with pooling only in the $x$ and $y$ dimensions.
The output after this final pooling step is flattened into a 1-D vector which is input into two fully connected layers.
The final layer produces a probability with a sigmoid activation function~\cite{funahashi1989approximate}.
All of the other activation functions are LeakyReLU~\cite{maas2013rectifier}.
We use a stochastic gradient descent optimizer with Nesterov's accelerated gradient~\cite{nesterov1983method}.
There are dropouts of $0.2$ after every pooling layer and the first dense layer, and a dropout of $0.5$ after the final dense layer to prevent overfitting.
We discuss all other network parameters in Section~\ref{sec:network-parameters}.

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.95\linewidth]{figures/architecture.png}
	\caption{The architecture for the neural networks follows the \textit{VGG} style of double convolutions followed by a max pooling operation. The number of filters doubles each layer leading to a fully connected layer and a sigmoid activation function.}
	\label{fig:architecture}
\end{figure*}

\subsection{Graph Partitioning}

After constructing the graph structure we apply a graph-based segmentation strategy.
There are many graph partitioning minimization functions that provide different constraints on their output.
Neurons in the brain should be acyclic (i.e. the output shape should have a genus of zero).
We enforce this constraint by finding a multicut partition of the graph that generates a \textit{forest} on the nodes.
A forest is a partitioning of a graph into a set of trees (i.e. no segment has a cycle).
There are several heuristics that solve the multicut problem (which is NP-Hard).
We use method presented by Keuper et al. that produces a feasible solution by greedy additive edge contraction~\cite{keuper2015efficient}.
