% !TEX root =  paper.tex

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.45\linewidth]{./figures/variation_of_information-microns-train-600.png}
	\includegraphics[width=0.45\linewidth]{./figures/variation_of_information-microns-test-600.png}
	\includegraphics[width=0.45\linewidth]{./figures/variation_of_information-FlyEM-train-600.png}
	\includegraphics[width=0.45\linewidth]{./figures/variation_of_information-FlyEM-test-600.png}
	\caption{VI scores of our method (red) compared to the baseline segmentation (green) and an oracle (blue) that optimally partitions the graph based on ground truth.}
	\label{fig:variation-of-information}
\end{figure*}

\section{Results}

\subsection{Variation of Information Improvement}

\subsection{Error Metric}
\label{sec:variation-of-information}

We evaluate the performance of the different methods using the split version of variance of information~\cite{meila2003comparing} (VI-s). Given a ground truth labeling $GT$ and our automatically reconstructed segmentation $SG$.
Over and under-segmentation are quantified by the conditional entropy $H(GT | SG)$ and $H(SG | GT)$, respectively. Since we are measuring the entropy between two clusterings, better VI scores are closer to the origin.

In Fig.~\ref{fig:variation-of-information}, we show VI scores for the input over-segmentation of the Kasthuri data \hp{check} at different thresholds of agglomeration (green) compared to our method (red). We also show the comparison to an oracle (blue) that correctly partitions the graph from our algorithm based on ground truth. \hp{add discussion of the results}

%The oracles sees the graph from our algorithm and correctly partitions the graph based on the ground truth.
%TODO{Our methods decreases the split variation of information by a factor of XX\% and only increases the merge variation of information by a factor of XX\% on the test datasets.}

Fig.~\ref{fig:positive-results} shows successful merges on the Kasthuri Vol. 2. Several of these successful examples combine multiple consecutive segments. The third example in the figure shows the correction of an over-segmented dendrite.
Fig.~\ref{fig:negative-results} shows some failure cases. In two of these examples the algorithm correctly predicted several merges and made one error.
In the third example a catastrophic merge error \hp{check} in the initial segmentation propagated to the output. We now analyze how each major component of our method contributes to the final result.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{./figures/multicut-correct1.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-correct2.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-correct3.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-correct4.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-correct5.png}
	\caption{Correctly segmented neurons from our method.}
	\label{fig:positive-results}
\end{figure}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.85\linewidth]{./figures/multicut-incorrect1.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-incorrect2.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-incorrect3.png}
	\includegraphics[width=0.85\linewidth]{./figures/multicut-incorrect4.png}
	\caption{Errors made by our method.}
	\label{fig:negative-results}
\end{figure}


\subsection{Graph Creation}

\begin{table}
	\centering
	\small
	\begin{tabular}{c c c} \hline
		\textbf{Dataset} & \textbf{Baseline} & \textbf{After Pruning} \\ \hline
		Kasthuri Vol. 1 & 763 / 21242 (3.47\%) & 753 / 3459 (17.88\%) \\
		Kasthuri Vol. 2 & 1010 / 26073 (3.73\%) & 904 / 4327 (17.28\%) \\
		FlyEM Vol. 1 & 269 / 14875 (1.78\%) & 262 / 946 (21.69\%) \\
		FlyEM Vol. 2 & 270 / 16808 (1.58\%) & 285 / 768 (27.07\%)\\ \hline
	\end{tabular}
	\caption{The results of our skeleton graph pruning heuristic compared to the baseline segmentation.}
	\label{table:skeletonization}
\end{table}

Table \ref{table:skeletonization} shows the results of pruning the skeleton graph using the heuristic discussed in Sec.~\ref{sec:skeletonization}. The baseline algorithm considers all adjacent regions for merging. Our method removes a significant portion of these candidates while maintaining a large number of the true merge locations. This edge pruning is essential for the graph partitioning algorithm, which has a computational complexity dependence on the number of edges. Our pruning heuristic removes at least $6\times$ the number of edges between correctly split segments on all datasets, achieving a maximum removal ratio of $20\times$.

Equally important is the number of split errors that remain after pruning. These are the locations that we want to merge to create a more accurate reconstruction. For every dataset, the number of positive candidates \hp{what is a positive candidate?} remains relatively equal. \hp{equal to what?}
However, since our heuristic does not enforce an adjacency constraint of two regions when constructing edges in the graph, the difference does not indicate the number of examples excluded by pruning. In fact, our method finds a number of examples that are non-adjacent. Figure \ref{fig:skeleton-results} shows two example segments with split errors, one that our algorithm missed (top) and one that it identified (bottom), despite the fact that the split segments are not adjacent.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.85\linewidth]{./figures/merge_candidate1.png}
	\includegraphics[width=0.85\linewidth]{./figures/merge_candidate2.png}
	\caption{Example merge candidates.}
	\label{fig:skeleton-results}
\end{figure}


\subsection{Classification Performance}

Table \ref{table:classification} shows the precision and recall rates for all of the datasets. We can train the CNN on anisotropic data and get good results on isotropic data. \hp{add more discussion}

\begin{table}[h]
	\centering
	\begin{tabular}{c c c c} \hline
		\textbf{Dataset} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} \\ \hline
		Kasthuri Training & 0.919 & 0.936 & 0.974 \\
		Kasthuri Testing & 0.737 & 0.717 & 0.907 \\
		FlyEM Vol. 1 & 0.796 & 0.478 & 0.862 \\
		FlyEM Vol. 2 & 0.762 & 0.422 & 0.810 \\ \hline
	\end{tabular}
	\caption{Precision and recall rates for the training and three test datasets.}
	\label{table:classification}
\end{table}

\subsection{Graph Based Strategies}

Applying a graph-based optimization strategy increases the accuracy over the CNN alone. In our test scores, we note an increase in precision on all datasets. For our segmentation problems we prefer a higher precision since it is more difficult to correct merge-errors. Table \ref{table:multicut} shows the changes in precision, recall, and accuracy for all three datasets in relation to the CNN without graph-based optimization. \hp{add more discussion}

\begin{table}[h]
	\centering
	\begin{tabular}{c c c c} \hline
		\textbf{Dataset} & $\Delta$ \textbf{Precision} & $\Delta$ \textbf{Recall} & $\Delta$ \textbf{Accuracy} \\ \hline
		Kasthuri Training & +3.60\% & -0.01\% & +0.60\% \\
		Kasthuri Testing & +7.59\% & -1.77\% & +1.38\% \\
		FlyEM Vol. 1 & +2.68\% & +0.76\% & +0.66\% \\
		FlyEM Vol. 2 & +2.22\% & -1.05\% & +0.29\% \\ \hline
	\end{tabular}
	\caption{Precision and recall for the training and three test datasets.}
	\label{table:multicut}
\end{table}
