\section{Results}

\subsection{Pruning via Skeletonization}

\begin{table}
	\centering
	\small
	\begin{tabular}{c c c} \hline
		\textbf{Dataset} & \textbf{Baseline} & \textbf{After Pruning} \\ \hline
		Kasthuri Vol. 1 & 763 / 21242 (3.47\%) & 753 / 3459 (17.88\%) \\
		Kasthuri Vol. 2 & 1010 / 26073 (3.73\%) & 904 / 4327 (17.28\%) \\
		FlyEM Vol. 1 & 269 / 14875 (1.78\%) & 262 / 946 (21.69\%) \\
		FlyEM Vol. 2 & 270 / 16808 (1.58\%) & 285 / 768 (27.07\%)\\ \hline
	\end{tabular}
	\caption{The results of our pruning heuristic compared to the current baseline.}
	\label{table:skeletonization}
\end{table}

Table \ref{table:skeletonization} shows the results of pruning using the skeletonization heuristic.
The baseline algorithm considers all adjacent regions for merging. 
Our method removes a significant portion of these candidates while maintaining a large number of the true merge locations.
This edge pruning is essential for the graph partitioning algorithms which have a computational complexity dependence on the number of edges (CHECK). 
Our pruning heuristic removes at least $6\times$ the number of edges between correctly split segments on all datasets, achieving a maximum removal ratio of $20\times$. 
Equally important is the number of split errors that remain after pruning.
These are the locations that we want to merge to create a more accurate reconstruction.
For every dataset, the number of positive candidates remains relatively even. 
However, since our heuristic does not enforce an adjacency constraint of two regions when constructing edges in the graph, the difference does not indicate the number of examples excluded by pruning. 
In fact, our method finds a number of examples which are non-adjacent.
Figure \ref{fig:skeleton-results} shows two example segments which are split errors.
The top example our algorithm missed but the segments are adjacent. 
The bottom example our algorithm found despite the fact that they are not adjacent.

\begin{figure}[t!]
	\centering
	\includegraphics[width=0.85\linewidth]{./figures/merge_candidate1.png}
	\includegraphics[width=0.85\linewidth]{./figures/merge_candidate2.png}
	\caption{Example merge candidates.}
	\label{fig:skeleton-results}
\end{figure}


\subsection{Classification Performance}

\begin{table}
	\centering
	\begin{tabular}{c c c c} \hline
	\textbf{Dataset} & \textbf{Precision} & \textbf{Recall} & \textbf{Accuracy} \\ \hline
	Kasthuri Training & & & \\
	Kasthuri Testing & & & \\
	FlyEM Vol. 1 & & & \\
	FlyEM Vol. 2 & & & \\ \hline
	\end{tabular}
	\caption{Precision and recall for the training and three test datasets.}
	\label{fig:classification}
\end{table}

Table \ref{table:classification} shows the precision and recall for all of the datasets. Our methods are more general than traditional connectomics examples allowing us to train the network on an anisotropic dataset and get impressive results on an isotropic dataset. Our initial network does not take in any image data, only the segment shapes, so we merely extract the $1200 \textrm{nm}^3$ region from any dataset and rescale as input into the neural network. For each of the tables, the top left square gives the number of segments which should merge that we predict merge. The bottom right square gives the number of segments which should not merge that we predict should not merge. The bottom left quadrant indicates the amount of false positives, candidates which we predict merge which do not. The top right quadrant gives the amount of false negatives where we predict split although the candidates should merge. Figure \ref{fig:network-results} shows the receiver operating characteristic (ROC) curve for all four datasets. 

\begin{figure}
	\includegraphics[width=0.9\linewidth]{./figures/roc-microns-300-test.png}
	\caption{The receiver operating characteristic (ROC) curve for all four datasets.}
	\label{fig:network-results}
\end{figure}

\subsection{Graph Based Strategies}

Using a graph-based optimization strategy prevents XX segments from merging compared to the simple hierarchical agglomeration strategy with an optimal threshold cut off. Since correcting merge errors is significantly more difficult than correcting split errors, it is desirable to limit the number of false merges. The graph-based strategies significantly improve this error type. 

\subsection{Variation of Information Improvements}

The final metric for comparing two connectomics segmentations is computing the variation of information with respect to an expert-labeled ground truth dataset. Figure \ref{fig:variation-of-information} shows the results on the datasets compared to the baseline (green) and an oracle (blue). 

\begin{figure*}[t!]
	\centering
	\includegraphics[width=0.42\linewidth]{./figures/variation_of_information-train.png}
	\hspace{0.085\linewidth}
	\includegraphics[width=0.42\linewidth]{./figures/variation_of_information-test.png}
	\caption{The improvement on variation of information from the baseline NeuroProof segmentation (green). Results closer to the origin are better.}
	\label{fig:variation-of-information}
\end{figure*}